{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1if_dqG9PCqtIa1Cdd3z_whoRqWwq9kts","timestamp":1738101116507}],"toc_visible":true,"gpuType":"T4","authorship_tag":"ABX9TyMsI0J2ZWDWO54BJBYAoItk"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["## Classic Machine Learning Algorithms from Scratch\n","\n","This notebook implements several foundational machine learning algorithms from scratch using NumPy and matplotlib â€” without relying on external machine learning libraries like scikit-learn.  \n","It is designed as an educational exercise to understand how these algorithms work under the hood.\n"],"metadata":{"id":"_1VztUr96Rwy"}},{"cell_type":"markdown","source":["# **Part A - SMO Algorithm for SVM**"],"metadata":{"id":"g-iV5LoRPLyr"}},{"cell_type":"markdown","source":["In this section, we implement the SMO (Sequential Minimal Optimization) algorithm used to train a Support Vector Machine (SVM).  \n","The SMO algorithm optimizes the dual form of the SVM problem by updating pairs of Lagrange multipliers (`alpha`) iteratively.\n","\n","We support both a linear kernel and an RBF (Radial Basis Function) kernel.  \n","The algorithm includes kernel matrix precomputation, KKT condition checks, alpha updates, clipping, and bias calculation."],"metadata":{"id":"qqYwvlY160hT"}},{"cell_type":"code","source":["import numpy as np"],"metadata":{"id":"3o5rJcOnwnzb","executionInfo":{"status":"ok","timestamp":1742664651360,"user_tz":-120,"elapsed":86,"user":{"displayName":"ido amar","userId":"01133300011231872560"}}},"execution_count":1,"outputs":[]},{"cell_type":"code","source":["# Linear kernel function\n","def linear_kernel(x1, x2):\n","    return np.dot(x1, x2)\n","\n","# RBF kernel functions\n","def rbf_kernel(x1, x2, gamma):\n","    return np.exp(-gamma * np.linalg.norm(x1 - x2)**2)\n","\n","# Compute the error for a given sample i, can use linear and RBF kernel\n","def compute_error(i, X, y, a, b, kernel_matrix):\n","    return np.dot(a * y, kernel_matrix[:, i]) + b - y[i]\n","\n","# Clips the value of alpha to stay within the range [L, H]\n","def clip_alpha(a, L, H):\n","    if a <= L:\n","        return L\n","    elif a >= H:\n","        return H\n","    else:\n","        return a"],"metadata":{"id":"5W6dlQBmwtdT","executionInfo":{"status":"ok","timestamp":1742664651370,"user_tz":-120,"elapsed":4,"user":{"displayName":"ido amar","userId":"01133300011231872560"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# SMO algorithm\n","def smo_algorithm(X, y, C, tol=1e-3, max_iter=1000, kernel=linear_kernel):\n","    \"\"\"\n","    Trains a Support Vector Machine (SVM) using the Sequential Minimal Optimization (SMO) algorithm.\n","\n","    Parameters:\n","        X (ndarray): Training features of shape (n_samples, n_features)\n","        y (ndarray): Training labels of shape (n_samples,), with values in {-1, 1}\n","        C (float): Regularization parameter\n","        tol (float): Tolerance for stopping criterion\n","        max_iter (int): Maximum number of iterations\n","        kernel (function): Kernel function to compute similarity between samples\n","\n","    Returns:\n","        a (ndarray): Optimized Lagrange multipliers (alphas)\n","        b (float): Bias term of the decision boundary\n","    \"\"\"\n","    n_samples, n_features = X.shape\n","    a = np.zeros(n_samples)  # Initialize Lagrange multipliers\n","    b = 0  # Bias term\n","    iter_count = 0  # Count total iterations\n","    no_change_iter_count = 0  # Iterations without alpha changes\n","\n","    # Precompute the kernel matrix\n","    kernel_matrix = np.array([[kernel(X[i], X[j]) for j in range(n_samples)] for i in range(n_samples)])\n","\n","    while iter_count < max_iter:\n","        a_changed = 0  # Track number of alpha updates in this iteration\n","\n","        for i in range(n_samples):\n","            E1 = compute_error(i, X, y, a, b, kernel_matrix)\n","\n","            # Check if the sample violates KKT conditions\n","            if (y[i] * E1 < -tol and a[i] < C) or (y[i] * E1 > tol and a[i] > 0):\n","\n","                # Select j by maximize |E1 - E2|\n","                E = np.array([compute_error(k, X, y, a, b, kernel_matrix) for k in range(n_samples)])\n","                j_c = np.argsort(-np.abs(E - E1))  # Descending order of errors\n","                j = j_c[1] if j_c[0] == i else j_c[0] # Choose the second largest if the first is i\n","\n","                E2 = compute_error(j, X, y, a, b, kernel_matrix)\n","                a1_old, a2_old = a[i], a[j] # keep the old Lagrange multipliers\n","\n","                # Compute L and H for clipping\n","                if y[i] != y[j]: # use for diffrent labels (s!=1)\n","                    L = max(0, a[j] - a[i])\n","                    H = min(C, C + a[j] - a[i])\n","                else: # for same labels (s=1)\n","                    L = max(0, a[i] + a[j] - C)\n","                    H = min(C, a[i] + a[j])\n","\n","                if L == H:\n","                    continue\n","\n","                # Compute eta\n","                eta = kernel_matrix[i, i] + kernel_matrix[j, j] - 2 * kernel_matrix[i, j]\n","                if eta == 0:\n","                    continue\n","\n","                # Update alpha[j]\n","                a[j] += y[j] * (E1 - E2) / eta\n","                a[j] = clip_alpha(a[j], L, H) # apdate a2 by clipping\n","\n","                # Check for significant change in alpha[j]\n","                if abs(a[j] - a2_old) < tol:\n","                    continue\n","\n","                # Update alpha[i] based on alpha[j] change\n","                a[i] += y[i] * y[j] * (a2_old - a[j])\n","\n","                # Compute new bias terms\n","                b1 = b - E1 - y[i] * (a[i] - a1_old) * kernel_matrix[i, i] - y[j] * (a[j] - a2_old) * kernel_matrix[i, j]\n","                b2 = b - E2 - y[i] * (a[i] - a1_old) * kernel_matrix[i, j] - y[j] * (a[j] - a2_old) * kernel_matrix[j, j]\n","\n","                # Update the bias term based on support vectors\n","                if 0 < a[i] < C:\n","                    b = b1\n","                elif 0 < a[j] < C:\n","                    b = b2\n","                else:\n","                    b = (b1 + b2) / 2  # Select the average of b1 and b2\n","\n","                a_changed += 1  # Count changes\n","\n","        # Update sum of iteration\n","        iter_count += 1\n","\n","        # Update iteration count if no changes\n","        no_change_iter_count = no_change_iter_count + 1 if a_changed == 0 else 0\n","\n","        # Stop after 10 consecutive iterations without change\n","        if no_change_iter_count >= 10:\n","            break\n","\n","    return a, b"],"metadata":{"id":"8tyNlsrcSNMV","executionInfo":{"status":"ok","timestamp":1742664651378,"user_tz":-120,"elapsed":5,"user":{"displayName":"ido amar","userId":"01133300011231872560"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["# **Part B - SVM Training & Evaluation**"],"metadata":{"id":"Qs6kWWlQPXqf"}},{"cell_type":"markdown","source":["In this section, we apply the SMO algorithm to train binary SVM classifiers using both linear and RBF kernels on the Iris dataset.\n","\n","For each class in the dataset, we:\n","- Relabel the data for one-vs-rest classification\n","- Split the data into training, validation, and test sets\n","- Tune hyperparameters (C and gamma) using validation accuracy\n","- Train final models with best-found parameters\n","- Evaluate each model on the test set using accuracy, confusion matrix, and sensitivity\n","\n","The results are printed per class and per kernel."],"metadata":{"id":"xOxv4wEn7bQw"}},{"cell_type":"code","source":["from sklearn.datasets import load_iris\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import confusion_matrix, accuracy_score\n","from tqdm import tqdm"],"metadata":{"id":"RckeiqtDPzbf","executionInfo":{"status":"ok","timestamp":1742664655392,"user_tz":-120,"elapsed":4016,"user":{"displayName":"ido amar","userId":"01133300011231872560"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["# Predict function\n","def predict(X_train, y_train, X_test, a, b, kernel):\n","    # Compute kernel matrix\n","    kernel_matrix = np.array([[kernel(x_train, x_test) for x_test in X_test] for x_train in X_train])\n","    # Compute decisions\n","    decision = np.dot(kernel_matrix.T, (a * y_train)) + b\n","\n","    return np.sign(decision)\n","\n","\n","# Hyperparameter tuning function for C and gamma\n","def tune_parameters(X_train, y_train, X_val, y_val, C_values, gamma_values=None, kernel_type=None):\n","    \"\"\"\n","    Performs hyperparameter tuning for an SVM using validation accuracy.\n","\n","    Supports both linear and RBF kernels. Iterates over combinations of C and gamma\n","    (if applicable), trains the model, and selects the best parameters based on validation accuracy.\n","\n","    Parameters:\n","        X_train (ndarray): Training features\n","        y_train (ndarray): Training labels\n","        X_val (ndarray): Validation features\n","        y_val (ndarray): Validation labels\n","        C_values (list): List of C values to try\n","        gamma_values (list): List of gamma values (only for RBF)\n","        kernel_type (str): 'linear' or 'rbf'\n","\n","    Returns:\n","        best_params (dict): Dictionary containing best C and (if applicable) gamma\n","        best_accuracy (float): Best validation accuracy achieved\n","    \"\"\"\n","    best_params = None\n","    best_accuracy = 0\n","\n","    if kernel_type == \"linear\":\n","        kernel = linear_kernel\n","        for C in tqdm(C_values, desc=\"Tuning C for linear kernel\", ncols=80, position=0, leave=True):\n","            a, b = smo_algorithm(X_train, y_train, C, kernel=kernel)\n","            y_pred = predict(X_train, y_train, X_val, a, b, kernel)\n","            accuracy = accuracy_score(y_val, y_pred)\n","            if accuracy > best_accuracy: # Chose the best accuracy\n","                best_accuracy = accuracy\n","                best_params = {'C': C}\n","    elif kernel_type == \"rbf\" and gamma_values is not None: # Check each \"C\" with each gamma\n","        for C in tqdm(C_values, desc=\"Tuning C for RBF kernel\", ncols=80, position=0, leave=True):\n","            for gamma in tqdm(gamma_values, desc=f\"Tuning gamma for C={C}\", ncols=80, position=1, leave=False):\n","                kernel = lambda x1, x2: rbf_kernel(x1, x2, gamma)\n","                a, b = smo_algorithm(X_train, y_train, C, kernel=kernel)\n","                y_pred = predict(X_train, y_train, X_val, a, b, kernel)\n","                accuracy = accuracy_score(y_val, y_pred)\n","                if accuracy > best_accuracy: # Chose the best accuracy\n","                    best_accuracy = accuracy\n","                    best_params = {'C': C, 'gamma': gamma}\n","    else:\n","        raise ValueError(\"Invalid kernel type or missing gamma values for RBF\")\n","\n","    return best_params, best_accuracy\n","\n","# Function to train and tune models for each class\n","def train_and_tune_models(X, y):\n","    \"\"\"\n","    Trains one-vs-rest binary SVM classifiers for each class using SMO.\n","\n","    For each class in the dataset:\n","    - Relabels the dataset as binary (+1 vs -1)\n","    - Splits data into train, validation, and test\n","    - Tunes hyperparameters (C for linear, C and gamma for RBF)\n","    - Trains final models with best parameters and stores them\n","\n","    Parameters:\n","        X (ndarray): Input features\n","        y (ndarray): Class labels\n","\n","    Returns:\n","        binary_models_linear (list): List of trained models using linear kernel\n","        binary_models_rbf (list): List of trained models using RBF kernel\n","        X_train (ndarray): Last used training features\n","        y_train (ndarray): Last used training labels\n","        X_test (ndarray): Last used test features\n","        y_test (ndarray): Last used test labels\n","    \"\"\"\n","    unique_classes = np.unique(y)\n","    binary_models_linear = []\n","    binary_models_rbf = []\n","\n","    for class_label in tqdm(unique_classes, desc=\"\\nTraining models for each class\", ncols=80, position=0, leave=True):\n","        print(f\"\\nTraining model for class {class_label} vs others:\")\n","\n","        # Relabel the dataset: current class as +1, others as -1\n","        y_binary = np.where(y == class_label, 1, -1)\n","\n","        # Split the dataset into train, validation, and test sets (60%, 20%, 20%)\n","        X_train, X_temp, y_train, y_temp = train_test_split(X, y_binary, test_size=0.4, random_state=42)\n","        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","        # Tune the hyperparameters C for each kernel and gamma for RBF kernel\n","        C_values = [0.1, 1, 10]\n","        gamma_values = [0.01, 0.1, 1]  # Only used for RBF kernel\n","\n","        # Tuning for linear kernel\n","        best_params_linear, best_accuracy_linear = tune_parameters(X_train, y_train, X_val, y_val, C_values, kernel_type=\"linear\")\n","\n","        # Tuning for RBF kernel\n","        best_params_rbf, best_accuracy_rbf = tune_parameters(X_train, y_train, X_val, y_val, C_values, gamma_values=gamma_values, kernel_type=\"rbf\")\n","\n","        # Print results of the two kernels\n","        print(f\"\\n--Best params for linear kernel for class {class_label}: {best_params_linear} (Validation Accuracy: {best_accuracy_linear})\")\n","        print(f\"--Best params for RBF kernel for class {class_label}: {best_params_rbf} (Validation Accuracy: {best_accuracy_rbf})\\n\")\n","\n","        # Train the model with the best params for linear kernel\n","        kernel_linear = linear_kernel\n","        a_linear, b_linear = smo_algorithm(X_train, y_train, best_params_linear['C'], kernel=kernel_linear)\n","\n","        # Store the model parameters for linear kernel\n","        binary_models_linear.append((a_linear, b_linear, best_params_linear['C']))\n","\n","        # Train the model with the best params for RBF kernel\n","        kernel_rbf = lambda x1, x2: rbf_kernel(x1, x2, best_params_rbf['gamma'])\n","        a_rbf, b_rbf = smo_algorithm(X_train, y_train, best_params_rbf['C'], kernel=kernel_rbf)\n","\n","        # Store the model parameters for RBF kernel\n","        binary_models_rbf.append((a_rbf, b_rbf, best_params_rbf['C'], best_params_rbf['gamma']))\n","\n","    return binary_models_linear, binary_models_rbf, X_train, y_train, X_test, y_test"],"metadata":{"id":"zbV89iA5VMrl","executionInfo":{"status":"ok","timestamp":1742664655416,"user_tz":-120,"elapsed":17,"user":{"displayName":"ido amar","userId":"01133300011231872560"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Load the Iris dataset\n","data = load_iris()\n","X, y = data.data, data.target\n","\n","# Training phase\n","binary_models_linear, binary_models_rbf, X_train, y_train, X_test, y_test = train_and_tune_models(X, y)"],"metadata":{"id":"DFG1Z68i6vw9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Visualization function\n","def visualize_models(binary_models_linear, binary_models_rbf, X, y):\n","    \"\"\"\n","    Evaluates and prints performance of trained SVM models on the test set.\n","\n","    For each class:\n","    - Relabels data for binary classification\n","    - Applies both linear and RBF models to the test set\n","    - Computes and displays confusion matrix, accuracy, and sensitivity\n","\n","    Parameters:\n","        binary_models_linear (list): Trained models with linear kernel\n","        binary_models_rbf (list): Trained models with RBF kernel\n","        X (ndarray): Original features\n","        y (ndarray): Original class labels\n","    \"\"\"\n","    unique_classes = np.unique(y)\n","    class_names = {0: \"Setosa\", 1: \"Versicolor\", 2: \"Virginica\"}  # Replace with actual class names if available\n","\n","    for idx, class_label in enumerate(unique_classes):\n","        class_name = class_names.get(class_label, f\"Class {class_label}\")\n","        print(f\"Results for {class_name} (Class {class_label}):\")\n","\n","        # Relabel the dataset: current class as +1, others as -1\n","        y_binary = np.where(y == class_label, 1, -1)\n","\n","        # Split the dataset into train, validation, and test sets\n","        X_train, X_temp, y_train, y_temp = train_test_split(X, y_binary, test_size=0.4, random_state=42)\n","        X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n","\n","        # Evaluate the model for linear kernel\n","        a_linear, b_linear, C_linear = binary_models_linear[idx]\n","        y_test_pred_linear = predict(X_train, y_train, X_test, a_linear, b_linear, linear_kernel)\n","        conf_matrix_linear = confusion_matrix(y_test, y_test_pred_linear)\n","        accuracy_linear = accuracy_score(y_test, y_test_pred_linear)\n","\n","        # Calculate TP, TN, FP, FN and sensitivity for linear kernel\n","        TP_linear = conf_matrix_linear[1, 1]\n","        TN_linear = conf_matrix_linear[0, 0]\n","        FP_linear = conf_matrix_linear[0, 1]\n","        FN_linear = conf_matrix_linear[1, 0]\n","        sensitivity_linear = TP_linear / (TP_linear + FN_linear) if (TP_linear + FN_linear) > 0 else 0\n","\n","        print(\"Linear Kernel:\")\n","        print(\"Confusion Matrix:\")\n","        print(conf_matrix_linear)\n","        print(f\"Test Accuracy: {accuracy_linear * 100:.2f}%\")\n","        print(\"Table of Confusion:\")\n","        print(f\"TP={TP_linear}, TN={TN_linear}, FP={FP_linear}, FN={FN_linear}\")\n","        print(f\"Sensitivity: {sensitivity_linear * 100:.2f}%\\n\")\n","\n","        # Evaluate the model for RBF kernel\n","        a_rbf, b_rbf, C_rbf, gamma_rbf = binary_models_rbf[idx]\n","        kernel_rbf = lambda x1, x2: rbf_kernel(x1, x2, gamma_rbf)\n","        y_test_pred_rbf = predict(X_train, y_train, X_test, a_rbf, b_rbf, kernel_rbf)\n","        conf_matrix_rbf = confusion_matrix(y_test, y_test_pred_rbf)\n","        accuracy_rbf = accuracy_score(y_test, y_test_pred_rbf)\n","\n","        # Calculate TP, TN, FP, FN and sensitivity for RBF kernel\n","        TP_rbf = conf_matrix_rbf[1, 1]\n","        TN_rbf = conf_matrix_rbf[0, 0]\n","        FP_rbf = conf_matrix_rbf[0, 1]\n","        FN_rbf = conf_matrix_rbf[1, 0]\n","        sensitivity_rbf = TP_rbf / (TP_rbf + FN_rbf) if (TP_rbf + FN_rbf) > 0 else 0\n","\n","        print(\"RBF Kernel:\")\n","        print(\"Confusion Matrix:\")\n","        print(conf_matrix_rbf)\n","        print(f\"Test Accuracy: {accuracy_rbf * 100:.2f}%\")\n","        print(\"Table of Confusion:\")\n","        print(f\"TP={TP_rbf}, TN={TN_rbf}, FP={FP_rbf}, FN={FN_rbf}\")\n","        print(f\"Sensitivity: {sensitivity_rbf * 100:.2f}%\\n\")\n","\n","        print(\"-------------------\")\n","\n","# Visualization phase\n","visualize_models(binary_models_linear, binary_models_rbf, X, y)"],"metadata":{"id":"0QrFcayI3omr","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742664684371,"user_tz":-120,"elapsed":123,"user":{"displayName":"ido amar","userId":"01133300011231872560"}},"outputId":"f83fc0b2-42e3-4de4-a07a-86b57a0ec89c"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["Results for Setosa (Class 0):\n","Linear Kernel:\n","Confusion Matrix:\n","[[19  0]\n"," [ 0 11]]\n","Test Accuracy: 100.00%\n","Table of Confusion:\n","TP=11, TN=19, FP=0, FN=0\n","Sensitivity: 100.00%\n","\n","RBF Kernel:\n","Confusion Matrix:\n","[[19  0]\n"," [ 0 11]]\n","Test Accuracy: 100.00%\n","Table of Confusion:\n","TP=11, TN=19, FP=0, FN=0\n","Sensitivity: 100.00%\n","\n","-------------------\n","Results for Versicolor (Class 1):\n","Linear Kernel:\n","Confusion Matrix:\n","[[11  6]\n"," [ 0 13]]\n","Test Accuracy: 80.00%\n","Table of Confusion:\n","TP=13, TN=11, FP=6, FN=0\n","Sensitivity: 100.00%\n","\n","RBF Kernel:\n","Confusion Matrix:\n","[[17  0]\n"," [ 1 12]]\n","Test Accuracy: 96.67%\n","Table of Confusion:\n","TP=12, TN=17, FP=0, FN=1\n","Sensitivity: 92.31%\n","\n","-------------------\n","Results for Virginica (Class 2):\n","Linear Kernel:\n","Confusion Matrix:\n","[[24  0]\n"," [ 1  5]]\n","Test Accuracy: 96.67%\n","Table of Confusion:\n","TP=5, TN=24, FP=0, FN=1\n","Sensitivity: 83.33%\n","\n","RBF Kernel:\n","Confusion Matrix:\n","[[24  0]\n"," [ 0  6]]\n","Test Accuracy: 100.00%\n","Table of Confusion:\n","TP=6, TN=24, FP=0, FN=0\n","Sensitivity: 100.00%\n","\n","-------------------\n"]}]},{"cell_type":"markdown","source":["# Summary"],"metadata":{"id":"VuO7n2So8FGY"}},{"cell_type":"markdown","source":["In this final section, we summarize and compare the classification results of the SVM models trained with linear and RBF kernels on the Iris dataset.\n","\n","### Key Observations:\n","\n","- **Setosa (Class 0):**  \n","  Both linear and RBF kernels achieved perfect classification, with 100% accuracy and sensitivity.\n","\n","- **Versicolor (Class 1):**  \n","  - The **linear kernel** performed well with 80% accuracy and 100% sensitivity, but produced several false positives.  \n","  - The **RBF kernel** significantly improved performance, reaching 96.67% accuracy and 92.31% sensitivity.\n","\n","- **Virginica (Class 2):**  \n","  - The **linear kernel** achieved 96.67% accuracy and 83.33% sensitivity.  \n","  - The **RBF kernel** again reached perfect performance: 100% accuracy and sensitivity.\n","\n","### Conclusion:\n","\n","The RBF kernel consistently outperformed or matched the linear kernel across all classes, particularly for more complex boundaries (e.g., Class 1 and 2).  \n","This demonstrates the power of the RBF kernel in handling non-linearly separable data in multi-class classification tasks.\n","\n"],"metadata":{"id":"Cd0ANpOq8N-B"}}]}